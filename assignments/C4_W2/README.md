# Text Summarization

- Accept [C4_W2](https://classroom.github.com/a/32OK8yRX)

---

## Pre-trained Model

Pre-trained model is provided and could be download from [google drive](https://drive.google.com/drive/folders/1ghRMIO2XVc6nsIon5Q5OTcZUINgbJPyd?usp=share_link)

- `model.weights.npy.gz`: put it under `./C4_W2/` folder.

Note: do **NOT** push pre-train model and dataset files to github repo, otherwise autograder can not download your repo.

## Lab

- Attention

  In this notebook you'll explore the three ways of attention (encoder-decoder attention, causal attention, and bi-directional self attention) and how to implement the latter two with dot product attention.
  
- The Transformer Decoder

  In this notebook you'll explore the transformer decoder and how to implement it with trax.

## Assignment

- Transformer Summarizer
